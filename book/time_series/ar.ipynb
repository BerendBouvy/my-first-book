{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(AR)=\n",
    "# AR process\n",
    "\n",
    "The main goal is to introduce the AutoRegressive (AR) model to describe a **stationary stochastic process**. Hence the AR model can be applied on time series where e.g. trend and seasonality are not present / removed, and only noise remains, or after applying other methods [to obtain a stationary time series](stationarize).\n",
    "\n",
    "## Process definition\n",
    "\n",
    "In an AR model, we forecast the variable of interest using a linear combination of its past values. A zero mean AR process of orders $p$ can be written as follows:\n",
    "\n",
    "$$S_t = \\overbrace{\\beta_1S_{t-1}+...+\\beta_pS_{t-p}}^{\\text{AR process}} + e_t $$ \n",
    "\n",
    "or as\n",
    "\n",
    "$$S_t = \\sum_{i=1}^p \\beta_iS_{t-i}+e_t$$\n",
    "\n",
    "Each observation is made up of a **random error** $e_t$ at that epoch, a linear combination of **past observations**. The errors $e_t$  are uncorrelated purely random noise process, known also as white noise. We note the process should still be stationary, satisfying\n",
    "\n",
    "$$\\mathbb{E}(S_t)=0, \\hspace{20px} \\mathbb{D}(S_t)=\\sigma^2,\\quad \\forall t$$\n",
    "\n",
    "This indicates that parts of the total variability of the process come from the signal and noise of past epochs, and only a (small) portion belongs to the noise of that epoch (denoted as $e_t$). To have a better understanding of the process itself, we consider two special cases, $q=0$ and $p=0$.\n",
    "\n",
    "### First-order AR(1) process\n",
    "\n",
    "We will just focus on explaining $p=1$, i.e. the AR(1) process. A **zero-mean first order autoregressive** process can be written as follows\n",
    "\n",
    "$$S_t = \\beta S_{t-1}+e_t, \\hspace{20px} -1\\leq\\beta<1, \\hspace{20px} t=2,...,m$$\n",
    "\n",
    "where $e_t$ is an i.i.d. noise process, e.g. distributed as $e_t\\sim N(0,\\sigma_{e}^2)$. See later the definition of $\\sigma_{e}^2$.\n",
    "\n",
    ":::{card} Exercise\n",
    "\n",
    "In a zero-mean first order autoregressive process, abbreviated as AR(1), we have $m=3$ observations, $\\beta=0.8$, and the generated white noise errors are $e = [e_1,\\, e_2,\\, e_3]^T=[1,\\, 2,\\, -1]^T$. What is the generated AR(1) process $S = [S_1,\\, S_2,\\, S_3]^T$?\n",
    "\n",
    "a. $S = \\begin{bmatrix}1 & 2.8 & 1.24\\end{bmatrix}^T$  \n",
    "b. $S = \\begin{bmatrix} 0 & 2 & 0.6 \\end{bmatrix}^T$  \n",
    "c. $S = \\begin{bmatrix} 1 & 2 & -1 \\end{bmatrix}^T$  \n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "The correct answer is **a**. The AR(1) process can be initialized as $S_1=e_1=1$. The next values can be obtained through:\n",
    "\n",
    "$$\n",
    "S_t = \\beta S_{t-1} + e_t\n",
    "$$\n",
    "\n",
    "Giving $S_2=0.8 S_1 + e_2 = 0.8\\cdot 1 + 2 = 2.8$ and $S_3=0.8 S_2 + e_3 = 0.8\\cdot 2.8 - 1= 1.24$, so we have:\n",
    "\n",
    "$$\n",
    "S = \n",
    "\\begin{bmatrix}1 & 2.8 & 1.24\\end{bmatrix}^T \n",
    "$$\n",
    "\n",
    "```\n",
    ":::\n",
    "\n",
    "**Formulation**\n",
    "\n",
    "Initializing $S_1=e_1$, with $\\mathbb{E}(S_1)=\\mathbb{E}(e_1)=0$ and $\\mathbb{D}(S_1)=\\mathbb{D}(e_1)=\\sigma^2$. Following this, multiple applications of the above \"autoregressive\" formula ($S_t = \\beta S_{t-1} + e_t$) gives:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_1&=e_1\\\\ \n",
    "S_2&=\\beta S_1+e_2\\\\ \n",
    "S_3 &= \\beta S_2+e_3 = \\beta^2S_1+\\beta e_2+e_3\\\\ \n",
    "&\\vdots\\\\ \n",
    "S_m &= \\beta S_{m-1} + e_m = \\beta^{m-1}S_1+\\beta^{m-2}e_2+...+\\beta e_{m-1}+e_m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "of which we still have (in order to impose the *stationarity*):\n",
    "\n",
    "$$\\mathbb{E}(S_t)=0 \\hspace{5px}\\text{and}\\hspace{5px} \\mathbb{D}(S_t)=\\sigma^2, \\hspace{10px} t=1,...,m$$\n",
    "\n",
    "All the error components, $e_t$, are uncorrelated such that $Cov(e_t,e_{t+\\tau})=0$ if $\\tau \\neq 0$, and with variance $\\sigma_{e}^2$ which still needs to be determined.\n",
    "\n",
    "**Autocovariance**\n",
    "\n",
    "The mean of the process is zero and, therefore:\n",
    "\n",
    "$$\\mathbb{E}(S_t) = \\mathbb{E}(\\beta S_{t-1}+e_t) = \\beta\\mathbb{E}(S_{t-1})+\\mathbb{E}(e_t) = 0$$\n",
    "\n",
    "The variance of the process should remain constant as:\n",
    "\n",
    "$$\\mathbb{D}(S_t) = \\mathbb{D}(\\beta S_{t-1} +e_t) \\Leftrightarrow \\sigma^2=\\beta^2\\sigma^2+\\sigma_{e}^2, \\hspace{10px} t\\geq 2$$\n",
    "\n",
    "resulting in\n",
    "\n",
    "$$\\sigma_{e}^2 = \\sigma^2 (1-\\beta^2)$$\n",
    "\n",
    "indicating that $\\sigma_{e}^2$ is smaller than $\\sigma^2$.\n",
    "\n",
    "The autocovariance (covariance between $S_t$ and $S_{t+\\tau}$) is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_{\\tau}&=\\mathbb{E}(S_t S_{t+\\tau})-\\mu^2 =\\mathbb{E}(S_t S_{t+\\tau})\\\\\n",
    "&= \\mathbb{E}(S_t(\\beta^\\tau S_t + \\beta^{\\tau-1} e_{t+1}+...)) = \\beta^\\tau\\mathbb{E}(S_t^2)=\\sigma^2\\beta^\\tau\n",
    "\\end{align*}$$\n",
    "\n",
    "In the derivation above we used that:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_{t+\\tau}=\\beta^\\tau S_t + \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and the fact that $S_t$ and $e_{t+\\tau}$ are uncorrelated for $\\tau \\neq 0$.\n",
    "\n",
    "```{admonition} Derivation (optional)\n",
    ":class: tip, dropdown\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_{t+\\tau}&= \\beta^{t+\\tau-1}S_1 + \\beta^{t+\\tau-2}e_2+...+ \\beta^{\\tau} e_{t}+ \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\\\\\n",
    "&= \\beta^{\\tau} \\left(\\beta^{t-1}S_1 + \\beta^{t-2}e_2+...+  e_{t}\\right)+ \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\\\\\n",
    "&=\\beta^\\tau S_t + \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "**Model structure of AR(1)**\n",
    "\n",
    "$$\\mathbb{E}(S) = \\mathbb{E}\\begin{bmatrix}S_1\\\\ S_2\\\\ \\vdots\\\\ S_m\\end{bmatrix} = \\begin{bmatrix}0\\\\ 0\\\\ \\vdots\\\\ 0\\end{bmatrix}, \\hspace{15px} \\mathbb{D}(S)=\\Sigma_{S}=\\sigma^2 \\begin{bmatrix}1&\\beta&...&\\beta^{m-1}\\\\ \\beta&1&...&\\beta^{m-2}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ \\beta^{m-1}&\\beta^{m-2}&...&1\\end{bmatrix}$$\n",
    "\n",
    "* Autocovariance function $\\implies$ $c_{\\tau}=\\sigma^2\\beta^\\tau$\n",
    "* Normalized autocovariance function (ACF) $\\implies$ $\\rho_\\tau=c_{\\tau}/c_0=\\beta^\\tau$\n",
    "* Larger value of $\\beta$ indicates a long-memory random process\n",
    "* If $\\beta=0$, this is called *purely random process* (white noise)\n",
    "* ACF is even, $c_{\\tau}=c_{-\\tau}=c_{|\\tau|}$ and so is $\\rho_{\\tau}=\\rho_{-\\tau}=\\rho_{|\\tau|}$\n",
    "\n",
    "Later in this section we will see how the coefficient $\\beta$ can be estimated.\n",
    "\n",
    "**Simulated example**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "auto-execute-page",
     "thebe-remove-input-init"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2194b2dd27b4a87802edadc0f98a608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.8, description='Phi:', max=1.0, min=-1.0), Output()), _dom_classes=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "def acfplot(phi=0.8):\n",
    "    # Parameters for the AR(1) process\n",
    "\n",
    "    sigma = 1          # Standard deviation of the noise\n",
    "    n = 500            # Length of the time series\n",
    "\n",
    "    # Initialize the process\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.zeros(n)\n",
    "    X[0] = np.random.normal(0, sigma)  # Initial value\n",
    "\n",
    "    # Generate the AR(1) process and noise series\n",
    "    noise = np.random.normal(0, sigma, n)  # Pre-generate noise for the second plot\n",
    "    for t in range(1, n):\n",
    "        X[t] = phi * X[t-1] + noise[t]  # Use pre-generated noise\n",
    "\n",
    "    # Create the 2x1 subplot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=False)\n",
    "\n",
    "    # Plot the AR(1) process in the first subplot\n",
    "    ax1.plot(X, label=f\"AR(1) Process with φ={round(phi, 2)}\")\n",
    "    ax1.set_ylabel(\"Value\")\n",
    "    ax1.set_title(\"Simulated AR(1) Process\")\n",
    "    ax1.set_ylim(-4, 4)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot the white noise in the second subplot\n",
    "    lags = 20\n",
    "    plot_acf(X, ax=ax2, lags=lags, title=\"ACF of White Noise\")\n",
    "    ax2.set_xlabel(\"Time\")\n",
    "    ax2.set_ylabel(\"ACF Value\")\n",
    "    ax2.set_title(\"ACF of AR(1) Process\")\n",
    "    ax2.set_xlim(-0.5, lags)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interact(acfplot, phi=widgets.FloatSlider(value=0.8, min=-1, max=1.0, step=0.1, description='Phi:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of coefficients of AR process\n",
    "\n",
    "If the values of $p$ and of the AR($p$) process are known, the question is: **how can we estimate the coefficients $\\beta_1,...,\\beta_p$**\n",
    "\n",
    "Here, we only elaborate on AR(1) using best linear unbiased estimation (BLUE) to estimate $\\beta_1$. The method can be generalized to estimate the parameters of an AR($p$) process.\n",
    "\n",
    "**Example: Parameter estimation of AR(1)**\n",
    "\n",
    "The AR(1) process is of the form\n",
    "\n",
    "$$S_t=\\beta_1 S_{t-1}+e_t$$\n",
    "\n",
    "In order to estimate the $\\beta_i$ we can set up the following linear model of observation equations (starting from $t=3$):\n",
    "\n",
    "$$\\begin{bmatrix}S_2 \\\\ S_3 \\\\ \\vdots \\\\ S_m \\end{bmatrix} = \\begin{bmatrix}S_1 \\\\S_2 \\\\ \\vdots\\\\ S_{m-1} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\end{bmatrix} + \\begin{bmatrix}e_{2} \\\\ e_{3}\\\\ \\vdots \\\\ e_{m} \\end{bmatrix}$$\n",
    "\n",
    "The BLUE estimator of $\\beta$ is given by:\n",
    "\n",
    "$$\\hat{\\beta}=(\\mathrm{A}^T\\mathrm{A})^{-1}\\mathrm{A}^TS$$\n",
    "\n",
    "Where $\\mathrm{A}=\\begin{bmatrix}S_1 & S_2 & \\cdots & S_{m-1}\\end{bmatrix}^T$ and $S=\\begin{bmatrix}S_2 & S_3 & \\cdots & S_m\\end{bmatrix}^T$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAMude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
