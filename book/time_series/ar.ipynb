{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(AR)=\n",
    "# AR process\n",
    "\n",
    "The main goal is to introduce the AutoRegressive (AR) model to describe a **stationary stochastic process**. Hence the AR model can be applied on time series where e.g. trend and seasonality are not present / removed, and only noise remains, or after applying other methods [to obtain a stationary time series](stationarize).\n",
    "\n",
    "## Process definition\n",
    "\n",
    "In an AR model, we forecast the variable of interest using a linear combination of its past values. A zero mean AR process of orders $p$ can be written as follows:\n",
    "\n",
    "$$S_t = \\overbrace{\\beta_1S_{t-1}+...+\\beta_pS_{t-p}}^{\\text{AR process}} + e_t $$ \n",
    "\n",
    "or as\n",
    "\n",
    "$$S_t = \\sum_{i=1}^p \\beta_iS_{t-i}+e_t$$\n",
    "\n",
    "Each observation is made up of a **random error** $e_t$ at that epoch, a linear combination of **past observations**. The errors $e_t$  are uncorrelated purely random noise process, known also as white noise. We note the process should still be stationary, satisfying\n",
    "\n",
    "$$\\mathbb{E}(S_t)=0, \\hspace{20px} \\mathbb{D}(S_t)=\\sigma^2,\\quad \\forall t$$\n",
    "\n",
    "This indicates that parts of the total variability of the process come from the signal and noise of past epochs, and only a (small) portion belongs to the noise of that epoch (denoted as $e_t$). To have a better understanding of the process itself, we consider two special cases, $q=0$ and $p=0$.\n",
    "\n",
    "### First-order AR(1) process\n",
    "\n",
    "We will just focus on explaining $p=1$, i.e. the AR(1) process. A **zero-mean first order autoregressive** process can be written as follows\n",
    "\n",
    "$$S_t = \\beta S_{t-1}+e_t, \\hspace{20px} -1\\leq\\beta<1, \\hspace{20px} t=2,...,m$$\n",
    "\n",
    "where $e_t$ is an i.i.d. noise process, e.g. distributed as $e_t\\sim N(0,\\sigma_{e}^2)$. See later the definition of $\\sigma_{e}^2$.\n",
    "\n",
    ":::{card} Exercise\n",
    "\n",
    "In a zero-mean first order autoregressive process, abbreviated as AR(1), we have $m=3$ observations, $\\beta=0.8$, and the generated white noise errors are $e = [e_1,\\, e_2,\\, e_3]^T=[1,\\, 2,\\, -1]^T$. What is the generated AR(1) process $S = [S_1,\\, S_2,\\, S_3]^T$?\n",
    "\n",
    "a. $S = \\begin{bmatrix}1 & 2.8 & 1.24\\end{bmatrix}^T$  \n",
    "b. $S = \\begin{bmatrix} 0 & 2 & 0.6 \\end{bmatrix}^T$  \n",
    "c. $S = \\begin{bmatrix} 1 & 2 & -1 \\end{bmatrix}^T$  \n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "The correct answer is **a**. The AR(1) process can be initialized as $S_1=e_1=1$. The next values can be obtained through:\n",
    "\n",
    "$$\n",
    "S_t = \\beta S_{t-1} + e_t\n",
    "$$\n",
    "\n",
    "Giving $S_2=0.8 S_1 + e_2 = 0.8\\cdot 1 + 2 = 2.8$ and $S_3=0.8 S_2 + e_3 = 0.8\\cdot 2.8 - 1= 1.24$, so we have:\n",
    "\n",
    "$$\n",
    "S = \n",
    "\\begin{bmatrix}1 & 2.8 & 1.24\\end{bmatrix}^T \n",
    "$$\n",
    "\n",
    "```\n",
    ":::\n",
    "\n",
    "**Formulation**\n",
    "\n",
    "Initializing $S_1=e_1$, with $\\mathbb{E}(S_1)=\\mathbb{E}(e_1)=0$ and $\\mathbb{D}(S_1)=\\mathbb{D}(e_1)=\\sigma^2$. Following this, multiple applications of the above \"autoregressive\" formula ($S_t = \\beta S_{t-1} + e_t$) gives:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_1&=e_1\\\\ \n",
    "S_2&=\\beta S_1+e_2\\\\ \n",
    "S_3 &= \\beta S_2+e_3 = \\beta^2S_1+\\beta e_2+e_3\\\\ \n",
    "&\\vdots\\\\ \n",
    "S_m &= \\beta S_{m-1} + e_m = \\beta^{m-1}S_1+\\beta^{m-2}e_2+...+\\beta e_{m-1}+e_m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "of which we still have (in order to impose the *stationarity*):\n",
    "\n",
    "$$\\mathbb{E}(S_t)=0 \\hspace{5px}\\text{and}\\hspace{5px} \\mathbb{D}(S_t)=\\sigma^2, \\hspace{10px} t=1,...,m$$\n",
    "\n",
    "All the error components, $e_t$, are uncorrelated such that $Cov(e_t,e_{t+\\tau})=0$ if $\\tau \\neq 0$, and with variance $\\sigma_{e}^2$ which still needs to be determined.\n",
    "\n",
    "**Autocovariance**\n",
    "\n",
    "The mean of the process is zero and, therefore:\n",
    "\n",
    "$$\\mathbb{E}(S_t) = \\mathbb{E}(\\beta S_{t-1}+e_t) = \\beta\\mathbb{E}(S_{t-1})+\\mathbb{E}(e_t) = 0$$\n",
    "\n",
    "The variance of the process should remain constant as:\n",
    "\n",
    "$$\\mathbb{D}(S_t) = \\mathbb{D}(\\beta S_{t-1} +e_t) \\Leftrightarrow \\sigma^2=\\beta^2\\sigma^2+\\sigma_{e}^2, \\hspace{10px} t\\geq 2$$\n",
    "\n",
    "resulting in\n",
    "\n",
    "$$\\sigma_{e}^2 = \\sigma^2 (1-\\beta^2)$$\n",
    "\n",
    "indicating that $\\sigma_{e}^2$ is smaller than $\\sigma^2$.\n",
    "\n",
    "The autocovariance (covariance between $S_t$ and $S_{t+\\tau}$) is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_{\\tau}&=\\mathbb{E}(S_t S_{t+\\tau})-\\mu^2 =\\mathbb{E}(S_t S_{t+\\tau})\\\\\n",
    "&= \\mathbb{E}(S_t(\\beta^\\tau S_t + \\beta^{\\tau-1} e_{t+1}+...)) = \\beta^\\tau\\mathbb{E}(S_t^2)=\\sigma^2\\beta^\\tau\n",
    "\\end{align*}$$\n",
    "\n",
    "In the derivation above we used that:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_{t+\\tau}=\\beta^\\tau S_t + \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and the fact that $S_t$ and $e_{t+\\tau}$ are uncorrelated for $\\tau \\neq 0$.\n",
    "\n",
    "```{admonition} Derivation (optional)\n",
    ":class: tip, dropdown\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_{t+\\tau}&= \\beta^{t+\\tau-1}S_1 + \\beta^{t+\\tau-2}e_2+...+ \\beta^{\\tau} e_{t}+ \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\\\\\n",
    "&= \\beta^{\\tau} \\left(\\beta^{t-1}S_1 + \\beta^{t-2}e_2+...+  e_{t}\\right)+ \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\\\\\n",
    "&=\\beta^\\tau S_t + \\beta^{\\tau-1} e_{t+1}+...+e_{t+\\tau}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "**Model structure of AR(1)**\n",
    "\n",
    "$$\\mathbb{E}(S) = \\mathbb{E}\\begin{bmatrix}S_1\\\\ S_2\\\\ \\vdots\\\\ S_m\\end{bmatrix} = \\begin{bmatrix}0\\\\ 0\\\\ \\vdots\\\\ 0\\end{bmatrix}, \\hspace{15px} \\mathbb{D}(S)=\\Sigma_{S}=\\sigma^2 \\begin{bmatrix}1&\\beta&...&\\beta^{m-1}\\\\ \\beta&1&...&\\beta^{m-2}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ \\beta^{m-1}&\\beta^{m-2}&...&1\\end{bmatrix}$$\n",
    "\n",
    "* Autocovariance function $\\implies$ $c_{\\tau}=\\sigma^2\\beta^\\tau$\n",
    "* Normalized autocovariance function (ACF) $\\implies$ $\\rho_\\tau=c_{\\tau}/c_0=\\beta^\\tau$\n",
    "* Larger value of $\\beta$ indicates a long-memory random process\n",
    "* If $\\beta=0$, this is called *purely random process* (white noise)\n",
    "* ACF is even, $c_{\\tau}=c_{-\\tau}=c_{|\\tau|}$ and so is $\\rho_{\\tau}=\\rho_{-\\tau}=\\rho_{|\\tau|}$\n",
    "\n",
    "Later in this section we will see how the coefficient $\\beta$ can be estimated.\n",
    "\n",
    "**Simulated example**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "auto-execute-page",
     "thebe-remove-input-init"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb398c27e1ac43109d134a8c00a214af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.8, description='Phi:', max=1.0, min=-1.0), Output()), _dom_classes=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "def acfplot(phi=0.8):\n",
    "    # Parameters for the AR(1) process\n",
    "\n",
    "    sigma = 1          # Standard deviation of the noise\n",
    "    n = 500            # Length of the time series\n",
    "\n",
    "    # Initialize the process\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.zeros(n)\n",
    "    X[0] = np.random.normal(0, sigma)  # Initial value\n",
    "\n",
    "    # Generate the AR(1) process and noise series\n",
    "    noise = np.random.normal(0, sigma, n)  # Pre-generate noise for the second plot\n",
    "    for t in range(1, n):\n",
    "        X[t] = phi * X[t-1] + noise[t]  # Use pre-generated noise\n",
    "\n",
    "    # Create the 2x1 subplot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 4), sharex=False)\n",
    "\n",
    "    # Plot the AR(1) process in the first subplot\n",
    "    ax1.plot(X, label=f\"AR(1) Process with φ={round(phi, 2)}\")\n",
    "    ax1.set_ylabel(\"Value\")\n",
    "    ax1.set_title(\"Simulated AR(1) Process\")\n",
    "    ax1.set_ylim(-4, 4)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot the white noise in the second subplot\n",
    "    lags = 20\n",
    "    plot_acf(X, ax=ax2, lags=lags, title=\"ACF of White Noise\")\n",
    "    ax2.set_xlabel(\"Time\")\n",
    "    ax2.set_ylabel(\"ACF Value\")\n",
    "    ax2.set_title(\"ACF of AR(1) Process\")\n",
    "    ax2.set_xlim(-0.5, lags)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interact(acfplot, phi=widgets.FloatSlider(value=0.8, min=-1, max=1.0, step=0.1, description='Phi:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of coefficients of ARMA process\n",
    "\n",
    "If the values of $p$ and $q$ of the ARMA($p,q$) process are known, the question is: **how can we estimate the coefficients $\\beta_1,...,\\beta_p$ and $\\theta_1,...,\\theta_q$?**\n",
    "\n",
    "Here, we only elaborate on AR(2) = ARMA(2,0) using best linear unbiased estimation (BLUE) to estimate $\\beta_1$ and $\\beta_2$. The method can be generalized to estimate the parameters of an ARMA($p,q$) process.\n",
    "\n",
    "**Example: Parameter estimation of AR(2)**\n",
    "\n",
    "The AR(2) process is of the form\n",
    "\n",
    "$$S_t=\\beta_1 S_{t-1}+\\beta_2 S_{t-2}+e_t$$\n",
    "\n",
    "In order to esitimate the $\\beta_i$ we can set up the following linear model of observation equations (starting from $t=3$):\n",
    "\n",
    "$$\\begin{bmatrix}S_3 \\\\ S_4 \\\\ \\vdots \\\\ S_m \\end{bmatrix} = \\begin{bmatrix}S_2 & S_1 \\\\S_3 & S_2\\\\ \\vdots & \\vdots\\\\ S_{m-1}&S_{m-2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\begin{bmatrix}e_{3} \\\\ e_{4}\\\\ \\vdots \\\\ e_{m} \\end{bmatrix}$$\n",
    "\n",
    "The BLUE estimator of $\\beta=[\\beta_1,\\beta_2]^T$ is\n",
    "\n",
    "$$\\hat{\\beta}=(\\mathrm{A}^T\\mathrm{A})^{-1}\\mathrm{A}^TS$$\n",
    "\n",
    "\n",
    "## Worked example - Single Differencing\n",
    "\n",
    "On this worked example, we will show that [single differencing](SD) induces an MA(1) process. The original time series is given as:\n",
    "\n",
    "$$Y=\\begin{bmatrix}Y_1\\\\ Y_2\\\\ \\vdots \\\\ Y_m\\end{bmatrix}, \\hspace{10px} \\Sigma_{Y}=\\sigma^2 I_m$$\n",
    "\n",
    "We apply single differencing which in this case results in a purely random process:\n",
    "\n",
    "$$\\begin{cases}S_1 = \\Delta Y_1 = Y_1\\\\ S_2=\\Delta Y_2 = Y_2 - Y_1\\\\ S_3=\\Delta Y_3 = Y_3-Y_2\\\\ \\quad\\vdots \\\\ S_m= \\Delta Y_m = Y_m - Y_{m-1}\\end{cases}$$\n",
    "\n",
    "In matrix notation, this can be written as:\n",
    "\n",
    "$$\\begin{bmatrix} S_1\\\\  S_2\\\\ \\vdots \\\\  S_m\\end{bmatrix} = \\underbrace{\\begin{bmatrix}\n",
    "    1 & 0 &   & \\dots & 0\\\\\n",
    "    -1 & 1 & 0 &   &  \\\\\n",
    "    0 & -1 & 1 & \\ddots & \\\\\n",
    "    \\vdots & \\ddots &\\ddots & \\ddots & 0 \\\\\n",
    "    0 & \\dots & 0 & -1 & 1\n",
    "\\end{bmatrix}}_{\\mathrm{T}}\\begin{bmatrix}Y_1\\\\ Y_2\\\\ \\vdots \\\\ Y_m\\end{bmatrix} \\Longleftrightarrow S = \\mathrm{T}Y$$\n",
    "\n",
    "We apply the [variance propagation law](01_LinearProp):\n",
    "\n",
    "$$\\Sigma_{ S}=\\mathrm{T}\\Sigma_{Y}\\mathrm{T}^T = \\mathrm{T}\\sigma^2I_m\\mathrm{T}^T=\\sigma^2\\mathrm{TT}^T$$\n",
    "\n",
    "such that we obtain:\n",
    "\n",
    "$$\\Sigma_{S} = \\sigma^2\\mathrm{TT}^T = 2\\sigma^2\\begin{bmatrix}1&-0.5&0&\\dots&0\\\\ -0.5&1&-0.5& &\\\\ 0&-0.5&1&\\ddots&0\\\\ \\vdots& &\\ddots&\\ddots&-0.5\\\\ 0&\\dots&0&-0.5&1\\end{bmatrix}$$\n",
    "\n",
    "We can see that the structure indeed corresponds with the covariance matrix of an AR(1) process, from which we see that $\\rho_1=-0.5$. Now we can find the value of $\\theta$: \n",
    "\n",
    "$$\\begin{cases}\\rho_1=-0.5=\\frac{\\theta}{1+\\theta^2}\\\\  S_t = \\theta e_{t-1}+e_t\\end{cases}\\implies \\theta=-1 \\implies S_t = e_t-e_{t-1}$$\n",
    "\n",
    ":::{card} Exercise\n",
    "\n",
    "For the stationary AR(2) process, calculate the ACF at lag 1. In other words, calculate $\\rho_1$.\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "For the AR($p$) process we know that $\\mathbb{E}(S_t)=0$, and $Var(S_t)=\\sigma^2$ ($\\forall t$), and\n",
    "\n",
    "$$S_t = \\beta_1S_{t-1}+\\beta_2S_{t-2}+e_t=\n",
    "\\begin{bmatrix}\\beta_1 & \\beta_2 & 1\\end{bmatrix}\\begin{bmatrix}S_{t-1} \\\\ S_{t-2} \\\\ e_t\\end{bmatrix}$$\n",
    "\n",
    "To compute the autocovariance function at lag 1, $c_1$, we need to compute the covariance between $S_{t-1}$ and $S_t$, which is given as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_1 &= \\mathbb{E}(S_{t-1}S_t)\n",
    "= \\mathbb{E}\\left(S_{t-1}\n",
    "(\\beta_1 S_{t-1} + \\beta_2 S_{t-2} + e_t)\n",
    "\\right)\n",
    "\\\\\n",
    "&= \\beta_1 \\mathbb{E}(S_{t-1}^2)\n",
    "+ \\beta_2 \\mathbb{E}(S_{t-2}S_{t-1})\n",
    "+ \\mathbb{E}(S_{t-1}e_t)\\\\\n",
    "&= \\beta_1 \\sigma^2\n",
    "+ \\beta_2 c_1\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "which gives\n",
    "\n",
    "$$\n",
    "\\beta_1 \\sigma^2 = c_1(1-\\beta_2)\n",
    "$$\n",
    "\n",
    "or, because $\\rho_1=c_1/\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\rho_1=\\frac{\\beta_1}{1-\\beta_2}\n",
    "$$\n",
    "\n",
    "```\n",
    ":::\n",
    "\n",
    "## Brief Summary\n",
    "\n",
    "The random processes (noise processes) considered here are:\n",
    "\n",
    "* ARMA($p,q$) process\n",
    "\n",
    "$$\n",
    "S_t = \\sum_{i=1}^p \\beta_iS_{t-i}+e_t+\\sum_{i=1}^q\\theta_ie_{t-1}\n",
    "$$\n",
    "\n",
    "* AR($p$) process\n",
    "\n",
    "$$\n",
    "S_t = \\sum_{i=1}^p \\beta_iS_{t-i}+e_t\n",
    "$$\n",
    "\n",
    "* MA($q$) process\n",
    "\n",
    "$$\n",
    "S_t = e_t+\\sum_{i=1}^q\\theta_ie_{t-1}\n",
    "$$\n",
    "\n",
    "The parameters of these stochastic processes can be estimated using the least-squares method. This allows then to predict the stochastic process, needed for [forecasting](forecast)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAMude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
